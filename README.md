# ğŸš€ AI Speech Summarizer Agent  
### *RAG-Based Â· Fully Local Â· Powered by Ollama + Mistral + ChromaDB*

This project is a **production-style AI Speech Summarization Agent** capable of reading long text files, chunking them into meaningful segments, retrieving relevant context via semantic vector search, and generating clean, context-aware summaries using **Mistral LLM served locally through Ollama**.

Everything runs inside **Docker Compose**, making the system fully reproducible, portable, and free from Python environment conflicts.

---

# ğŸ§± Project Structure (Flat Layout)

```
AmbedkarGPT-Intern-Task/
â”‚
â”œâ”€â”€ main.py              # Main application logic (RAG pipeline)
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ speech.txt           # Input file for summarization
â”œâ”€â”€ docker-compose.yml   # Multi-container setup (App + Chroma)
â”œâ”€â”€ Dockerfile           # Image build for Python app
â”œâ”€â”€ .dockerignore
â””â”€â”€ README.md
```

---

# ğŸ§  Key Features

- Reads any `.txt` based document or long speech  
- Splits text into meaningful semantic chunks  
- Generates embeddings using Sentence-Transformers  
- Stores vectors in **ChromaDB** for similarity search  
- Retrieves the most relevant chunks using vector similarity  
- Summarizes using **Mistral LLM** running locally via Ollama  
- Entire pipeline runs through Docker Compose  
- Zero cloud dependency â†’ full privacy â†’ local execution  

---

# ğŸ›  Tech Stack

- Python  
- LangChain  
- Sentence-Transformers  
- ChromaDB  
- Ollama (Mistral LLM)  
- Docker & Docker Compose  
- Git  

---

# âš ï¸ Prerequisites (IMPORTANT)

Before running the project, make sure you have:

### âœ” Docker Installed  
https://www.docker.com/products/docker-desktop/

### âœ” Git Installed  
https://git-scm.com/downloads

### âœ” Ollama Installed (Runs on host machine, NOT in Docker)  
https://ollama.com/download

Install Mistral model:

```
ollama pull mistral
```

---

# ğŸ“¥ Clone the Repository

```
git clone https://github.com/Busted-pinch/AmbedkarGPT-Intern-Task.git
cd AmbedkarGPT-Intern-Task
```

---

# âœï¸ Preparing Input File

Replace or edit:

```
speech.txt
```

- Must be `.txt`  
- Can be extremely long  
- Add any text you want the agent to summarize  

---

# â–¶ï¸ Execution Steps (Super Descriptive â€” Follow Carefully)

## **1ï¸âƒ£ Start Ollama**

Before touching Docker, run:

```
ollama serve
```

You should see:

```
Listening on 127.0.0.1:11434
```

Leave it running.

---

## **2ï¸âƒ£ Start Docker Compose**

Inside the project folder:

```
sudo docker compose up --build
```

This performs:

- Builds the Python app image  
- Installs dependencies inside container  
- Starts ChromaDB  
- Connects Python app â†’ ChromaDB  
- Connects Python app â†’ Ollama (host)  
- Runs the full RAG summarization pipeline  

---

## **3ï¸âƒ£ Behind the Scenes Workflow**

- Reads `speech.txt`  
- Chunks text  
- Generates embeddings  
- Stores vectors in ChromaDB  
- Performs similarity search  
- Sends prompt to Mistral through Ollama  
- Prints final summary in logs  

You will see:

```
--- SUMMARY ---
<your final summary here>
```

---

## **4ï¸âƒ£ Stop Containers**

```
CTRL + C
sudo docker compose down
```

---

# ğŸ§¹ Troubleshooting

### ğŸ”¸ Ollama not responding  
Run:
```
ollama serve
```

### ğŸ”¸ Model not found  
Run:
```
ollama pull mistral
```

### ğŸ”¸ Chroma issues  
Rebuild:
```
sudo docker compose down
sudo docker compose up --build
```

---

# ğŸ¯ Expected Output

A clean, readable, context-aware summary of your input text powered by:

- Retrieval-Augmented Generation  
- Local Vector Search  
- Local LLM Inference  

Everything happens entirely offline.

---

# ğŸ”— Repository

https://github.com/Busted-pinch/AmbedkarGPT-Intern-Task
